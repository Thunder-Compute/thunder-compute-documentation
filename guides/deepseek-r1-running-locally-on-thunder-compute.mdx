---
title: "Run DeepSeek R1 Affordably"
description: "Run DeepSeek R1 affordably on Thunder Compute. This guide shows how to set up an A100 GPU instance and use Ollama for cost-effective model deployment."
mode: "wide"
sidebarTitle: "DeepSeek R1"
---

# Easily Run DeepSeek R1 on Thunder Compute

Looking for the **cheapest way to run DeepSeek R1** or just want to **try DeepSeek R1** without buying hardware? Thunder Compute lets you spin up pay‑per‑minute A100 GPUs so you only pay for the time you use. Follow the steps below to get the model running in minutes.

> **Quick reminder:** Make sure your Thunder Compute account is set up. If not, start with our [Quickstart Guide](/quickstart).

If you prefer video instructions, watch this overview:

<iframe width="640" height="360" src="https://www.youtube.com/embed/EukG6P4s5QI?si=Sx3iWsISL8Ve58Uz" title="YouTube video player" frameborder="0" allowfullscreen />

## Step 1: Create a Cost‑Effective GPU Instance

Open your CLI and launch an 80 GB A100 GPU (perfect for the 70B variant):

```bash
tnr create --gpu "a100xl" --template "ollama"
```

For details on instance templates, see our [templates guide](/guides/using-instance-templates).

## Step 2: Check Status and Connect

Verify the instance is running:

```bash
tnr status
```

![Instance creation in CLI](/images/instance_creation_cli.png)

Connect with its ID:

```bash
tnr connect <instance-id>
```

## Step 3: Start the Ollama Server

Inside the instance, start Ollama:

```bash
start ollama
```

If you hit any hiccups, check our [troubleshooting guide](/docs/troubleshooting).

Wait about 30 seconds for the web UI to load.

![Ollama server startup](/images/start_ollama.png)

## Step 4: Access the Web UI and Load DeepSeek R1

1. Visit `http://localhost:8080` in your browser.
2. Choose **DeepSeek R1** from the dropdown. On an 80 GB A100, pick the **70B** variant for peak performance.

![Web UI with model selection](/images/web_ui_model_selection.png)

## Step 5: Run DeepSeek R1

Type a prompt in the web interface. For example:

> _"If the concepts of rCUDA were applied at scale, overcoming latency, what would it mean for the cost of GPUs on cloud providers?"_

The model will think through the answer and respond. A full reply can take up to 200 seconds.

![Model response in progress](/images/model_response_in_progress.png)

## Conclusion

That's the **cheapest way to run DeepSeek R1** and a quick way to **try DeepSeek R1** on Thunder Compute. Explore more guides:

- [Using Docker on Thunder Compute](/guides/using-docker-on-thundercompute)
- [Using Instance Templates](/guides/using-instance-templates)
- [Running Jupyter notebooks](/guides/running-jupyter-notebooks-on-thunder-compute)

Happy building\!