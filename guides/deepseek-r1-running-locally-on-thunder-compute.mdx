---
title: "DeepSeek R1 running locally on Thunder Compute"
description: "This guide explains how to run DeepSeek R1 locally on Thunder Compute"
mode: wide
sidebarTitle: "DeepSeek R1"
---

# Running Deepseek R1 Locally on Thunder Compute

In this guide, we'll show you how to run the Deepseek R1 model on Thunder Compute. To get the best performance I recommend using our 80GB A100 GPUs, which support the 70B model variant.

> **Note:** Make sure you’ve already set up your Thunder Compute account. If not, follow the steps in our [Quickstart Guide](https://thundercompute.com/docs/docs/quickstart).

---

If you prefer video guides, here is an overview of the following steps:

<iframe width="640" height="360" src="https://www.youtube.com/embed/EukG6P4s5QI?si=Sx3iWsISL8Ve58Uz" title="YouTube video player" frameborder="0" allowfullscreen></iframe>

## Step 1: Create Your GPU Instance

Open your CLI and run the following command to create an instance. For best performance, select an 80GB A100 GPU and opt for the 70B model variant.

```bash
tnr create --gpu "a100xl" --template "ollama"
```

---

## Step 2: Check Status and Connect

After creating your instance, check its status:

```bash
tnr status
```

![Instance creation in CLI](/images/instance_creation_cli.png)


Then, connect to your running instance using its ID:

```bash
tnr connect <instance-id>
```

---

## Step 3: Start the Ollama Server

Once connected, start the Ollama server (which also launches the web UI) by running:

```bash
start ollama
```

Wait around 30 seconds for the web UI to build and load.

![Ollama server startup](/images/start_ollama.png)

---

## Step 4: Access the Web UI and Load Deepseek R1

1. Open your web browser and navigate to:  
   `http://localhost:<YOUR_PORT>`  
   *(Replace `<YOUR_PORT>` with the appropriate port number.)*

2. In the web UI, select the Deepseek R1 model from the dropdown menu. If you’re on an 80GB A100 instance, be sure to pick the 70B variant for optimal performance.

<!-- Screenshot placeholder: Web UI with model selection -->

![Web UI with model selection](/images/web_ui_model_selection.png)
---

## Step 5: Interact with the Deepseek R1 Model

Type your prompt into the web interface. For example, try asking:

> *"What's a non-cringe way to sign off from a YouTube tutorial video?"*

After you submit your query, the model will begin processing. You’ll see its detailed thought process and response generation—note that it might take up to 200 seconds for a full answer to be produced.

![Model response in progress](/images/model_response_in_progress.png)

---

## Conclusion

That's it! You now have Deepseek R1 running locally on Thunder Compute. Enjoy exploring its detailed reasoning and powerful capabilities. For more in-depth tutorials, check out our other guides, including [using Docker on Thunder Compute](/guides/using-docker-on-thundercompute) and [using Instance Templates](/guides/using-instance-templates).
