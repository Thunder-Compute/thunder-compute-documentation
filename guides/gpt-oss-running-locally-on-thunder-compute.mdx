---
title: "Run GPT-OSS 120B on Thunder Compute"
description: "A Guide on how to setup and run gpt-oss 120b, affordably on Thunder Compute"
mode: "wide"
sidebarTitle: "GPT-OSS 120B"
---

# Run gpt-oss-120b on Thunder Compute

Looking for the **cheapest way to self-host and run gpt-oss-120b** or just want to **try gpt-oss-120b** without buying hardware? Thunder Compute lets you spin up pay‑per‑minute A100 GPUs so you only pay for the time you use. Follow the steps below to get the model running in minutes.

> **Quick reminder:** Make sure your Thunder Compute account is set up. If not, start with our [Quickstart Guide](/quickstart).

## Step 1: Create a Cost‑Effective prototyping mode GPU Instance

Open your CLI and launch an 80GB A100 GPU, with 80Gb of video ram it is large enough to host the full 120B model:

```bash
tnr create --gpu a100xl --vcpus 4 --mode prototyping --disk-size-gb 200 --template "ollama"
```

This will create one of our lower cost [prototyping mode](/prototyping-mode) instances with an A100 GPU, 4 vCPUs, and 200GB of storage, based off of our Ollama template.
This will generally be the most cost effective of self-hosting and running the gpt-oss-120b on Thunder Compute.

For more details on instance templates, see our [templates guide](/guides/using-instance-templates).

## Step 2: Check Status and Connect

Verify the instance is running:

```bash
tnr status
```

<img src="/images/InstanceStatus_v2.png" width="750" alt="Instance Status Result" />

```bash
tnr connect <instance-id>
```

## Step 3: Start the Ollama Server & download the model:

Inside the instance, start Ollama, this will start the Ollama server, as well as OpenWebUI, and a cloudflare tunnel to access the website on your local machine.

```bash
start-ollama
```

This can take a bit of time to spin up the web ui: we can start downloading the models early:

```bash
ollama pull gpt-oss:120b
```

If you hit any hiccups, check our [troubleshooting guide](/docs/troubleshooting).

Wait about 60 seconds for the web UI to load.

<img src="/images/Ollama_Status.png" width="750" alt="Ollama Status Result" />

## Step 4: Access the Web UI and Load gpt-oss:120b

1. Visit `http://localhost:8080` in your browser.
2. Choose **gpt-oss:120b** from the dropdown.

## Step 5: Run gpt-oss:120b

Type a prompt in the web interface. For example:

> _"Tell a tale of a seaman who found the treasure of the clouds by following the sound of thunder."_

## Conclusion

That's the **cheapest way to run gpt-oss:120b** and a quick way to **try gpt-oss:120b** on Thunder Compute. Explore more guides:

- [Using Docker on Thunder Compute](/guides/using-docker-on-thundercompute)
- [Using Instance Templates](/guides/using-instance-templates)
- [Running Jupyter notebooks](/guides/running-jupyter-notebooks-on-thunder-compute)

Happy building\!